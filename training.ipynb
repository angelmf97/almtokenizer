{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fd9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a444c9b",
   "metadata": {},
   "source": [
    "### Load the Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "config_path = \"config.yaml\"\n",
    "\n",
    "# Load the YAML\n",
    "with open(config_path, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "    os.makedirs(cfg[\"training\"][\"checkpoint_dir\"], exist_ok=True)\n",
    "    shutil.copy(config_path, cfg[\"training\"][\"checkpoint_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984fdb0e",
   "metadata": {},
   "source": [
    "### Define the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f204dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datasets import FSD50K, collate_fn_audio\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "dl_cfg = cfg[\"dataloader\"]\n",
    "batch_size = dl_cfg[\"batch_size\"]\n",
    "num_workers = dl_cfg[\"num_workers\"]\n",
    "nsecs = dl_cfg[\"nsecs\"]\n",
    "shuffle = dl_cfg[\"shuffle\"]\n",
    "train_subset_size = dl_cfg[\"train_subset_size\"]\n",
    "test_subset_size = dl_cfg[\"test_subset_size\"]\n",
    "dataset_path = dl_cfg[\"dataset_path\"]\n",
    "\n",
    "train_dataset = FSD50K(dataset_path, split=\"train\")\n",
    "test_dataset = FSD50K(dataset_path, split=\"test\")\n",
    "\n",
    "if train_subset_size is None:\n",
    "    train_subset_size = int(len(train_dataset))\n",
    "\n",
    "if test_subset_size is None:\n",
    "    test_subset_size = int(len(test_dataset))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dl = DataLoader(Subset(train_dataset, range(train_subset_size)), \n",
    "                batch_size=batch_size, \n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=lambda x: collate_fn_audio(x, nsecs=nsecs))\n",
    "\n",
    "test_dl = DataLoader(Subset(test_dataset, range(test_subset_size)), \n",
    "                batch_size=batch_size, \n",
    "                shuffle=shuffle,\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=lambda x: collate_fn_audio(x, nsecs=nsecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7ca4f",
   "metadata": {},
   "source": [
    "### Define the Model and Discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0fccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 5,146,624\n",
      "Discriminators parameters: 377,864\n",
      "Total parameters: 5,524,488\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from src.utils import load_model_from_config, load_discriminators_from_config\n",
    "from encodec.msstftd import MultiScaleSTFTDiscriminator\n",
    "import torchaudio\n",
    "\n",
    "from encodec.msstftd import DiscriminatorSTFT\n",
    "from encodec.modules.conv import NormConv2d\n",
    "from encodec.msstftd import get_2d_padding\n",
    "import typing as tp\n",
    "import torch.nn as nn\n",
    "\n",
    "def patched_init(self, filters: int, in_channels: int = 1, out_channels: int = 1,\n",
    "                n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, max_filters: int = 1024,\n",
    "                filters_scale: int = 1, kernel_size: tp.Tuple[int, int] = (3, 9), dilations: tp.List = [1, 2, 4],\n",
    "                stride: tp.Tuple[int, int] = (1, 2), normalized: bool = True, norm: str = 'weight_norm',\n",
    "                activation: str = 'LeakyReLU', activation_params: dict = {'negative_slope': 0.2}):\n",
    "    torch.nn.Module.__init__(self)\n",
    "    assert len(kernel_size) == 2\n",
    "    assert len(stride) == 2\n",
    "    self.filters = filters\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.n_fft = n_fft\n",
    "    self.hop_length = hop_length\n",
    "    self.win_length = win_length\n",
    "    self.normalized = normalized\n",
    "    self.activation = getattr(torch.nn, activation)(**activation_params)\n",
    "    self.spec_transform = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length, window_fn=torch.hann_window,\n",
    "        normalized=self.normalized, center=False, pad_mode=None, power=None)\n",
    "    spec_channels = 2 * self.in_channels\n",
    "    self.convs = nn.ModuleList()\n",
    "    self.convs.append(\n",
    "        NormConv2d(spec_channels, self.filters, kernel_size=kernel_size, padding=get_2d_padding(kernel_size))\n",
    "    )\n",
    "    in_chs = self.filters\n",
    "    for i, dilation in enumerate(dilations):\n",
    "        out_chs = min((filters_scale ** (i + 1)) * self.filters, max_filters)\n",
    "        self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride,\n",
    "                                        dilation=(dilation, 1), padding=get_2d_padding(kernel_size, (dilation, 1)),\n",
    "                                        norm=norm))\n",
    "        in_chs = out_chs\n",
    "    out_chs = min((filters_scale ** (len(dilations) + 1)) * self.filters, max_filters)\n",
    "    self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=(kernel_size[0], kernel_size[0]),\n",
    "                                    padding=get_2d_padding((kernel_size[0], kernel_size[0])),\n",
    "                                    norm=norm))\n",
    "    self.conv_post = NormConv2d(out_chs, self.out_channels,\n",
    "                                kernel_size=(kernel_size[0], kernel_size[0]),\n",
    "                                padding=get_2d_padding((kernel_size[0], kernel_size[0])),\n",
    "                                norm=norm)\n",
    "\n",
    "DiscriminatorSTFT.__init__ = patched_init\n",
    "\n",
    "\n",
    "model = load_model_from_config(cfg)\n",
    "discriminators = load_discriminators_from_config(cfg)\n",
    "\n",
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {model_params:,}\")\n",
    "\n",
    "disc_params = sum(p.numel() for p in discriminators.parameters() if p.requires_grad)\n",
    "print(f\"Discriminators parameters: {disc_params:,}\")\n",
    "\n",
    "print(f\"Total parameters: {model_params + disc_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8ab671",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85736873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9012b509e8c64ceab7d358091a3d8e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  7%|7         | 73/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691c74345e0e4fa783accf2e9e8a60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcb877b1b9045588b349d7b6b003818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation progress:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2187089e9b3e4be3a58353895d0492a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd92ade2581f4a0b9414678dbdb44aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5056298aabe54124ba7968960cbc5a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation progress:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96d7fd3628141deb6bac5249567df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b306c610caf84894bf8d1c81aab812dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9e6334a16f4b7fb0fba1498768fda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation progress:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38df07f2c1234b4181c80d39e9c84c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d943b58e7e24fee8360d3fe5c27f189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dc820ec0b34578b5f939ffa86d95a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation progress:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad2c5efea484937aaf46ce85f54ec0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa91592eef478ca66026e74cf55d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35eef0b497c4400784ae996e55e50ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation progress:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n",
      "warning: audio amplitude out of range, auto clipped.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef88417a4a84bc890bba5632fa55855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch progress:   0%|          | 0/1366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_cfg = cfg[\"training\"]\n",
    "num_epochs = training_cfg[\"num_epochs\"]\n",
    "discriminator_train_freq = training_cfg[\"discriminator_train_freq\"]\n",
    "d_train_prob = training_cfg[\"d_train_prob\"]\n",
    "checkpoint_freq = training_cfg[\"checkpoint_freq\"]\n",
    "eval_freq = training_cfg[\"eval_freq\"]\n",
    "start_checkpoint = training_cfg[\"start_checkpoint\"]\n",
    "writer_dir = training_cfg[\"writer_dir\"]\n",
    "checkpoint_dir = training_cfg[\"checkpoint_dir\"]\n",
    "checkpoint_dir = training_cfg[\"checkpoint_dir\"]\n",
    "lr_generator = training_cfg[\"lr_generator\"]\n",
    "lr_discriminator = training_cfg[\"lr_discriminator\"]\n",
    "weight_decay = training_cfg[\"weight_decay\"]\n",
    "betas = training_cfg[\"betas\"]\n",
    "lambdas = training_cfg[\"lambdas\"]\n",
    "\n",
    "model.train_model(train_dl=train_dl,\n",
    "                    test_dl=test_dl,\n",
    "                    discriminators=discriminators,\n",
    "                    num_epochs=num_epochs,\n",
    "                    discriminator_train_freq=discriminator_train_freq,\n",
    "                    d_train_prob=d_train_prob,\n",
    "                    checkpoint_freq=checkpoint_freq,\n",
    "                    start_checkpoint=start_checkpoint,\n",
    "                    eval_freq=eval_freq,\n",
    "                    writer_dir=writer_dir,\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    lr_g=lr_generator, \n",
    "                    weight_decay=weight_decay,\n",
    "                    lr_d=lr_discriminator,\n",
    "                    betas=betas,\n",
    "                    lambdas=lambdas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
