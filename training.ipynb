{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fd9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f204dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datasets import FSDKaggle2018Dataset, collate_fn_audio\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = FSDKaggle2018Dataset(\"../2552860\")\n",
    "dl = DataLoader(Subset(dataset, range(2048)), batch_size=16, shuffle=False, collate_fn=collate_fn_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fdb49f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALMTokenizer(\n",
      "  (query_encoder): QueryEncoder(\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pos_encoder): PositionalEncoding()\n",
      "  )\n",
      "  (query_decoder): QueryDecoder(\n",
      "    (transformer): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pos_decoder): PositionalEncoding()\n",
      "  )\n",
      "  (mae_encoder): QueryEncoder(\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pos_encoder): PositionalEncoding()\n",
      "  )\n",
      "  (mae_decoder): QueryDecoder(\n",
      "    (transformer): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pos_decoder): PositionalEncoding()\n",
      "  )\n",
      "  (pos_encoder): PositionalEncoding()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.model import ALMTokenizer\n",
    "\n",
    "encoder_args = {\"embed_dim\": 128, \"n_heads\": 8, \"n_layers\": 6}\n",
    "decoder_args = {\"embed_dim\": 128, \"n_heads\": 8, \"n_layers\": 6}\n",
    "\n",
    "mae_decoder_args = {\"embed_dim\": 128, \"n_heads\": 8, \"n_layers\": 4}\n",
    "mae_encoder_args = {\"embed_dim\": 128, \"n_heads\": 8, \"n_layers\": 4}\n",
    "\n",
    "patchify_args = {\"device\": \"cuda\"}\n",
    "unpatchify_args = {\"device\": \"cuda\"}\n",
    "\n",
    "model = ALMTokenizer(\n",
    "    from_raw_audio=True,\n",
    "    encoder_args=encoder_args,\n",
    "    decoder_args=decoder_args,\n",
    "    mae_decoder_args=mae_decoder_args,\n",
    "    mae_encoder_args=mae_encoder_args,\n",
    "    patchify_args=patchify_args,\n",
    "    unpatchify_args=unpatchify_args,\n",
    "    window_size=2,\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f1516af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.discriminator import Discriminator\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# 1) Define mel/log-mel transforms with each hop_length\n",
    "hop_lengths = [32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "mel_transforms = nn.ModuleList([\n",
    "    T.MelSpectrogram(sample_rate=24000, n_fft=1024, hop_length=h, win_length=1024)\n",
    "    for h in hop_lengths\n",
    "])\n",
    "\n",
    "# 2) Instantiate the 6 discriminators\n",
    "discriminators = nn.ModuleList([\n",
    "    Discriminator(\n",
    "        in_channels=128, \n",
    "        hidden_dims=[64,128,256,512,512,512], \n",
    "        mel_transform=m\n",
    "        ).to(device)\n",
    "    for m in mel_transforms\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import os\n",
    "\n",
    "writer_dir = \"runs/alm_tokenizer\"\n",
    "checkpoint_dir = \"checkpoints/alm_tokenizer\"\n",
    "checkpoint_freq = 10\n",
    "\n",
    "os.makedirs(writer_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/alm_tokenizer\")\n",
    "\n",
    "lr_g          = 1e-4\n",
    "weight_decay  = 1e-2\n",
    "num_epochs    = 200\n",
    "\n",
    "import torch.optim as optim\n",
    "optim_g = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr_g,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "from src.losses import compute_generator_loss, compute_discriminator_loss\n",
    "from itertools import chain\n",
    "\n",
    "lr_d = 2e-4\n",
    "betas = (0.5, 0.9)\n",
    "optim_d = optim.Adam(\n",
    "    params=chain(*[D.parameters() for D in discriminators]),\n",
    "    lr=lr_d,\n",
    "    betas=betas\n",
    ")\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    \n",
    "    losses = {\n",
    "        \"L_time\": 0.0,\n",
    "        \"L_freq\": 0.0,\n",
    "        \"L_adv\": 0.0,\n",
    "        \"L_feat\": 0.0,\n",
    "        \"L_mae\": 0.0,\n",
    "        \"L_total\": 0.0\n",
    "        }\n",
    "    \n",
    "    for i, wavs in enumerate(dl):\n",
    "        \n",
    "        wavs = wavs.to(device)\n",
    "\n",
    "        res = model(wavs)\n",
    "\n",
    "        x_hat = res[\"x_hat\"]\n",
    "        x = res[\"orig_waveform\"]\n",
    "        mae_pred = res[\"mae_pred\"]\n",
    "        mae_target = res[\"mae_target\"]\n",
    "        mask_idx = res[\"mask_indices\"]\n",
    "\n",
    "        # Late discriminator training\n",
    "        if epoch >= 10:\n",
    "            discriminator_loss = compute_discriminator_loss(discriminators, x, x_hat)\n",
    "            optim_d.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "        # Generator training\n",
    "        generator_loss = compute_generator_loss(\n",
    "            x_hat=x_hat,\n",
    "            x=x,\n",
    "            discriminators=discriminators,\n",
    "            mae_pred=mae_pred,\n",
    "            mae_target=mae_target,\n",
    "            mask_idx=mask_idx\n",
    "        )\n",
    "\n",
    "        for loss_type, loss_value in generator_loss.items():\n",
    "            losses[loss_type] = losses[loss_type] + loss_value.item()\n",
    "        \n",
    "        total_gen_loss = generator_loss[\"L_total\"]\n",
    "        \n",
    "        optim_g.zero_grad()\n",
    "        total_gen_loss.backward()\n",
    "        optim_g.step()\n",
    "\n",
    "    # Save the model every n epochs\n",
    "    if epoch % checkpoint_freq == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"alm_tokenizer_epoch_{epoch}.pth\"))\n",
    "        print(f\"Model saved at epoch {epoch}\")    \n",
    "\n",
    "    # Log losses\n",
    "    for loss_type, loss_value in losses.items():\n",
    "        losses[loss_type] /= len(wavs)\n",
    "        writer.add_scalar(f\"losses/{loss_type}\", losses[loss_type], epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch:2d} | Average Loss: {losses['L_total']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb64107",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(wavs)\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "ipd.display(ipd.Audio(res[\"x_hat\"][0].cpu().numpy(), rate=24000))\n",
    "ipd.display(ipd.Audio(res[\"orig_waveform\"][0].cpu().numpy(), rate=24000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
