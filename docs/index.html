<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>almtokenizer</title>
  <style>
    body { font-family: system-ui, sans-serif; margin: 40px; line-height: 1.6; max-width: 900px; }
    h1, h2, h3 { font-weight: 600; }
    pre { background: #f4f4f4; padding: 12px; border-radius: 4px; overflow-x: auto; }
    code { font-family: monospace; }
    table { border-collapse: collapse; margin: 20px 0; width: 100%; }
    td, th { border: 1px solid #ddd; padding: 12px; vertical-align: top; }
    audio { width: 100%; margin-top: 6px; }
  </style>
</head>
<body>

<h1>almtokenizer</h1>

<p>
This repository aims to reproduce the results of the paper 
<a href="https://arxiv.org/abs/2504.10344">"ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling"</a>. 
This work introduces a new way of compressing audio into discrete tokens that are both low-bitrate and semantically rich, 
making them more suitable for audio language modeling tasks such as text-to-speech, audio captioning, or music generation.
</p>

<h3>Quick setup</h3>
<p>Create and activate the conda environment using the following command:</p>
<pre><code>conda env create -f environment.yml
conda activate almtokenizer
</code></pre>

<p>
This environment uses python 3.10, torch 2.6.0 and CUDA toolkit 11.8 (a different version of CUDA toolkit may be needed depending on the machine).
</p>

<h3>Training</h3>
<p>
A config file is provided in <code>config.yaml</code> to facilitate defining and tracking the parameters of several configurations. 
Choose the desired parameters and run the <code>training.ipynb</code> notebook. 
The training process can then be monitored by running:
</p>
<pre><code>tensorboard --logdir runs/base_model    # Default directory, can be changed in the config
</code></pre>

<p>
The "scalars" tab allows monitoring of the training and evaluation losses, 
and the "audio" tab allows listening to a few pairs of original vs. reconstructed audios.
</p>

<p><b>Important:</b> Please make sure you have a copy of the FSD50K dataset and the path to the root directory of the dataset is correctly specified in the config file under <code>dataloader/dataset_path</code>.</p>

<p>
The window size (separation between CLS tokens) changes randomly for each batch, in a range between 2 and 10, so the model generalizes better for different values of window size.
</p>

<p>Most important parameters to check in the config:</p>
<pre><code>model:
    base_args:
        n_heads: number of attention heads of the transformer encoder and decoder.
        n_layers: number of transformer layers of the transformer encoder and decoder.

    mae_args:
        n_heads: similar to base_args, but for the MAE decoder.
        n_layers: similar to base_args, but for the MAE decoder.

discriminator:
    hop_lengths, n_fft, win_lengths: must be defined as lists.
    n_mels: number of mel filters.

dataloader:
    dataset_path: path to the root directory of FSD50K dataset.
    batch_size
    train_subset_size
    test_subset_size
    nsecs: desired audio length (e.g. 5 seconds).

training:
    num_epochs
    discriminator_train_freq
    checkpoint_freq
    eval_freq
    start_checkpoint
    writer_dir
    checkpoint_dir
    lr_generator
    lr_discriminator
    lambdas
</code></pre>

<h3>Sound Reconstruction Examples</h3>
<table>
  <tr>
    <td>
      <p>Original:</p>
      <audio controls src="audio/speech-female.wav"></audio>
    </td>
    <td>
      <p>Reconstructed (w=3):</p>
      <audio controls src="audio/reconstructed_window_3.wav"></audio>
    </td>
    <td>
      <p>Reconstructed (w=6):</p>
      <audio controls src="audio/reconstructed_window_6.wav"></audio>
    </td>
    <td>
      <p>Reconstructed (w=10):</p>
      <audio controls src="audio/reconstructed_window_10.wav"></audio>
    </td>
  </tr>
</table>

<h3>Sound Space Traversals</h3>
<table>
  <tr>
    <td><p>EnCodec:</p><audio controls src="audio/EnCodec_trajectory_0.wav"></audio></td>
    <td><p>ALMTokenizer:</p><audio controls src="audio/ALMTokenizer_trajectory_0.wav"></audio></td>
  </tr>
  <tr>
    <td><p>EnCodec:</p><audio controls src="audio/EnCodec_trajectory_1.wav"></audio></td>
    <td><p>ALMTokenizer:</p><audio controls src="audio/ALMTokenizer_trajectory_1.wav"></audio></td>
  </tr>
  <tr>
    <td><p>EnCodec:</p><audio controls src="audio/EnCodec_trajectory_2.wav"></audio></td>
    <td><p>ALMTokenizer:</p><audio controls src="audio/ALMTokenizer_trajectory_2.wav"></audio></td>
  </tr>
</table>

<h3>Zero-shot Timbre Transfer</h3>

<p>
In the following examples, we test whether we can change the timbre of a sound (its instrument-like quality) while keeping the pitch and rhythm intact.
</p>
<p>The idea is simple:</p>
<ol>
  <li>We encode an input sound into latent representations using both EnCodec and ALMTokenizer.</li>
  <li>For each instrument in the Good-sounds dataset, we compute an “average point” in latent space (a centroid).</li>
  <li>To transform a sound, we take its latent representation and shift it toward the centroid of a target instrument.</li>
  <li>Finally, we decode the shifted representation back into audio.</li>
</ol>

<p>
These examples show that while reconstruction quality is still limited, ALMTokenizer's latent space captures semantic structure more clearly. 
This makes the timbre transfer feel more intentional than with EnCodec, even if the results are far from perfect.
</p>

<h4> Male speech to cello </h4>

<table>
    <tr>
        <td><p>EnCodec before transfer:</p>
            <audio controls src="audio/before_encodec_1.wav"></audio>
        </td>
        <td><p>EnCodec after transfer:</p>
            <audio controls src="audio/after_encodec_1.wav"></audio>
        </td>
    </tr>
    <tr>
        <td><p>ALMTokenizer before transfer:</p>
            <audio controls src="audio/before_alm_1.wav"></audio>
        </td>
        <td><p>ALMTokenizer after transfer:</p>
            <audio controls src="audio/after_alm_1.wav"></audio>
        </td>
    </tr>
</table>

<h4> Female speech to cello </h4>

<table>
    <tr>
        <td><p>EnCodec before transfer:</p>
            <audio controls src="audio/before_encodec_2.wav"></audio>
        </td>
        <td><p>EnCodec after transfer:</p>
            <audio controls src="audio/after_encodec_2.wav"></audio>
        </td>
    </tr>
    <tr>
        <td><p>ALMTokenizer before transfer:</p>
            <audio controls src="audio/before_alm_2.wav"></audio>
        </td>
        <td><p>ALMTokenizer after transfer:</p>
            <audio controls src="audio/after_alm_2.wav"></audio>
        </td>
    </tr>
</table>

</body>
</html>
