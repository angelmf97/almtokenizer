{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3909fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ece83e",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07190a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "cfg_path = \"checkpoints/almtokenizer/config.yaml\"\n",
    "\n",
    "with open(cfg_path) as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "cfg[\"device\"] = device\n",
    "cfg[\"model\"][\"patchify_args\"][\"device\"] = device\n",
    "cfg[\"model\"][\"unpatchify_args\"][\"device\"] = device\n",
    "\n",
    "from src.utils import load_model_from_config\n",
    "\n",
    "model = load_model_from_config(cfg)\n",
    "\n",
    "# Find and load last epoch in the directory\n",
    "dir = os.path.join(cfg[\"training\"][\"checkpoint_dir\"], \"model\")\n",
    "last_epoch = max([int(f.split(\"_\")[1].removesuffix(\".pth\")) for f in os.listdir(dir) if f.startswith(\"epoch_\")], default=0)\n",
    "model.load_model(os.path.join(dir, f\"epoch_{last_epoch}.pth\"))\n",
    "print(last_epoch)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc6811",
   "metadata": {},
   "source": [
    "### Sound Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd952be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "wav_path = \"docs/audio/speech-female.wav\"  # Replace with your wav file path\n",
    "start_sec = 0\n",
    "\n",
    "waveform, sr = torchaudio.load(wav_path, normalize=True)\n",
    "\n",
    "waveform = torchaudio.functional.resample(\n",
    "                waveform, orig_freq=sr, new_freq=24000\n",
    "            )\n",
    "\n",
    "a = torch.tensor(waveform)\n",
    "\n",
    "# Convert a to mono keeping the channel dimension\n",
    "a = a.mean(dim=0, keepdim=True)\n",
    "x = a[None, :]\n",
    "print(x.shape)\n",
    "\n",
    "ipd.display(ipd.Audio(waveform, rate=24000))\n",
    "\n",
    "for w in [3, 6, 10]:\n",
    "    model.window_size = w\n",
    "    with torch.no_grad():\n",
    "        audio = x.to(device)\n",
    "    reconstructed = model(audio)[\"x_hat\"]\n",
    "\n",
    "    ipd.display(ipd.Audio(reconstructed.detach().cpu().numpy().flatten(), rate=24000))\n",
    "    torchaudio.save(f\"docs/audio/reconstructed_window_{w}.wav\", torch.tensor(reconstructed.detach().cpu().numpy()).squeeze(0), 24000)\n",
    "model.window_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f80a97",
   "metadata": {},
   "source": [
    "### Filter the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beaf7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "json_df = pd.read_json(\"../good_sounds_dataset/sounds.json\").T\n",
    "mask3 = json_df[\"klass\"] == \"good-sound\"\n",
    "\n",
    "valid_subset = json_df[mask3]\n",
    "\n",
    "takes = pd.read_json(\"../good_sounds_dataset/takes.json\").T\n",
    "db = valid_subset.merge(takes, left_on=\"id\", right_on=\"sound_id\")\n",
    "\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodec import EncodecModel\n",
    "import torch\n",
    "from utils import process_good_sounds_dataset\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "encodec_latents, encodec_attributes, alm_latents, alm_attributes = process_good_sounds_dataset(db, model, device=device, trim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17863662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import aggregate_latents\n",
    "\n",
    "X, df = aggregate_latents(encodec_latents, encodec_attributes, alm_latents, alm_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d36585",
   "metadata": {},
   "source": [
    "### Define the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_good_bad, generate_label_encoder\n",
    "\n",
    "# Transform labels to integer indices\n",
    "df[\"klass\"] = df[\"klass\"].apply(get_good_bad)\n",
    "attributes = [\"instrument\", \"note\", \"octave\", \"klass\"]\n",
    "label_encoders, num_labels = generate_label_encoder(df, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b343e1",
   "metadata": {},
   "source": [
    "### Define subsets for projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import stratified_sample\n",
    "\n",
    "indices = stratified_sample(\n",
    "    df, \n",
    "    source_col=\"source\",\n",
    "    n=10000,\n",
    "    group_cols=(\"instrument\", \"note\", \"octave\"),\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "df_subset = df.loc[indices]\n",
    "x_subset = X.loc[indices]\n",
    "num_labels_subset = num_labels.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f44765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from utils import projection\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"xlabel\": \"PCA 1\",\n",
    "    \"ylabel\": \"PCA 2\",\n",
    "}\n",
    "\n",
    "attributes = [\"instrument\", \"note\", \"octave\"]\n",
    "\n",
    "for method in x_subset.index.levels[0]:\n",
    "    plot_kwargs[\"suptitles\"] = f\"{method}\"\n",
    "    x_grp = x_subset.loc[method]\n",
    "    df_grp = df_subset.loc[method]\n",
    "    fig = projection(x_grp, df_grp, attributes, PCA, plot_kwargs=plot_kwargs)\n",
    "    fig.savefig(f\"figs/{method}_pca.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"xlabel\": \"t-SNE 1\",\n",
    "    \"ylabel\": \"t-SNE 2\",\n",
    "}\n",
    "\n",
    "for method in x_subset.index.levels[0]:\n",
    "    plot_kwargs[\"suptitles\"] = f\"{method}\"\n",
    "    x_grp = x_subset.loc[method]\n",
    "    df_grp = df_subset.loc[method]\n",
    "    fig = projection(x_grp, df_grp, attributes, TSNE, proj_fn_kwargs={'n_jobs': -1}, plot_kwargs=plot_kwargs)\n",
    "    fig.savefig(f\"figs/{method}_tsne.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"xlabel\": \"LDA 1\",\n",
    "    \"ylabel\": \"LDA 2\",\n",
    "}\n",
    "\n",
    "attributes = [\"instrument\", \"note\", \"octave\"]\n",
    "for method in x_subset.index.levels[0]:\n",
    "    plot_kwargs[\"suptitles\"] = f\"{method}\"\n",
    "    x_grp = x_subset.loc[method]\n",
    "    df_grp = df_subset.loc[method]\n",
    "    fig = projection(x_grp, df_grp, attributes, LDA, y=df_grp[attributes], plot_kwargs=plot_kwargs)\n",
    "    fig.savefig(f\"figs/{method}_lda.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8e99b",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score\n",
    "def scan_k(x, range):\n",
    "    aics = []\n",
    "    for k in range:\n",
    "        gmm = GaussianMixture(n_components=k, random_state=42).fit(x)\n",
    "        aics.append(gmm.aic(x))\n",
    "    best_k = list(range)[np.argmin(aics)]\n",
    "    return aics, best_k\n",
    "\n",
    "def compute_external_metrics(x, true_labels, k):\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42).fit(x)\n",
    "    pred_labels = gmm.predict(x)\n",
    "    return {\n",
    "        \"ARI\": adjusted_rand_score(true_labels, pred_labels),\n",
    "        \"mutual_info\": normalized_mutual_info_score(true_labels, pred_labels),\n",
    "        \"homogeneity\": homogeneity_score(true_labels, pred_labels),\n",
    "    }\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"xlabel\": \"t-SNE 1\",\n",
    "    \"ylabel\": \"t-SNE 2\",\n",
    "}\n",
    "\n",
    "for method in [\"EnCodec\", \"ALMTokenizer\"]:\n",
    "    range_k = trange(20, 60)\n",
    "\n",
    "    # Find the best number of clusters\n",
    "    aics, best_k = scan_k(x_subset.loc[method].sample(2000, random_state=42), range_k)\n",
    "    plt.figure()\n",
    "    plt.plot(list(range_k), aics)\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"AIC\")\n",
    "    plt.axvline(best_k, color='r', linestyle='--')\n",
    "    plt.title(f\"Model Selection: {method}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figs/{method}_k_scan.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Cluster with the optimal number of clusters\n",
    "    gmm = GaussianMixture(n_components=best_k, random_state=42)\n",
    "    clusters = gmm.fit_predict(x_subset.loc[method])\n",
    "\n",
    "\n",
    "\n",
    "    cluster_df = df_subset.loc[method].copy()\n",
    "    cluster_df[\"cluster\"] = clusters\n",
    "    attributes = [\"cluster\"]\n",
    "\n",
    "    fig = projection(x_subset.loc[method], cluster_df, attributes, proj_fn=TSNE, proj_fn_kwargs={'n_jobs': -1}, plot_kwargs=plot_kwargs)\n",
    "    fig.savefig(f\"figs/{method}_clusters.pdf\", bbox_inches='tight')\n",
    "    \n",
    "    three_attr = df_subset.loc[method, [\"instrument\", \"note\", \"octave\"]].astype(str).agg(\"_\".join, axis=1)\n",
    "    metrics = compute_external_metrics(x_subset.loc[method], three_attr, best_k)\n",
    "    print(f\"External metrics for {method}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec56c9f",
   "metadata": {},
   "source": [
    "# Linear Separability Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def linear_separability_test(label_encoders, x, num_labels, **kwargs):\n",
    "    for key in attributes:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, num_labels[key], test_size=0.2, random_state=42, stratify=num_labels[key])\n",
    "        svm = SVC(**kwargs)\n",
    "        svm.fit(X_train, y_train)\n",
    "        accuracy = svm.score(X_test, y_test)\n",
    "        print(f\"SVC accuracy for {key}: {accuracy:.2f}\")\n",
    "\n",
    "svc_kwargs = {'kernel': 'linear', 'C': 1e6, 'random_state': 42, 'max_iter': 1e5}\n",
    "attributes = [\"instrument\", \"note\", \"octave\"]\n",
    "for method in [\"EnCodec\", \"ALMTokenizer\"]:\n",
    "    print(f\"Testing linear separability for {method}\")\n",
    "    linear_separability_test(attributes, x_subset.loc[method], num_labels_subset.loc[method], **svc_kwargs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed346f",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def classification_test(attributes, x, num_labels, classifier, **classifier_kwargs):\n",
    "    for key in attributes:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, num_labels[key], test_size=0.2, random_state=42, stratify=num_labels[key])\n",
    "        clf = classifier(**classifier_kwargs)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracy = clf.score(X_test, y_test)\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        print(f\"Classifier accuracy for {key}: {accuracy:.2f}\")\n",
    "        print(f\"Classifier precision for {key}: {precision:.2f}\")\n",
    "\n",
    "rf_kwargs = {'n_estimators': 100, 'random_state': 42, 'n_jobs': -1}\n",
    "attributes = [\"instrument\", \"note\", \"octave\"]\n",
    "\n",
    "for method in [\"EnCodec\", \"ALMTokenizer\"]:\n",
    "    print(f\"Testing classification for {method}\")\n",
    "    classification_test(attributes, x_subset.loc[method], num_labels_subset.loc[method], RandomForestClassifier, **rf_kwargs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f55b41",
   "metadata": {},
   "source": [
    "# Interpolation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_good_sounds_dataset, aggregate_latents\n",
    "\n",
    "mask1 = pd.notnull(json_df[[\"sustain\", \"decay\", \"attack\"]]).sum(axis=1).astype(bool)\n",
    "mask2 = pd.notnull(json_df[[\"release\", \"offset\"]]).sum(axis=1).astype(bool)\n",
    "mask3 = json_df[\"klass\"] == \"good-sound\"\n",
    "\n",
    "valid_subset = json_df[mask1 & mask2 & mask3]\n",
    "db = valid_subset.merge(takes, left_on=\"id\", right_on=\"sound_id\")\n",
    "\n",
    "encodec_latents, encodec_attributes, alm_latents, alm_attributes = process_good_sounds_dataset(db, model, device=device, trim=True)\n",
    "X_synth, df_synth = aggregate_latents(encodec_latents, encodec_attributes, alm_latents, alm_attributes)\n",
    "df_synth.octave = df_synth.octave.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528330f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from utils import interpolate_latent, create_vectors\n",
    "\n",
    "\n",
    "trajectory1 = {\n",
    "    \"instrument\": [\"flute\", \"flute\", \"flute\"],\n",
    "    \"note\": [\"C\", \"E\", \"G\"],\n",
    "    \"octave\": [\"5\", \"5\", \"5\"],\n",
    "}\n",
    "\n",
    "trajectory2 = {\n",
    "    \"instrument\": [\"clarinet\", \"trumpet\"],\n",
    "    \"note\": [\"G\", \"G\"],\n",
    "    \"octave\": [\"5\", \"5\"],\n",
    "}\n",
    "\n",
    "trajectory3 = {\n",
    "    \"instrument\": [\"clarinet\", \"flute\"],\n",
    "    \"note\": [\"A\", \"A\"],\n",
    "    \"octave\": [\"5\", \"6\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "from encodec import EncodecModel\n",
    "encodec_dec = EncodecModel.encodec_model_24khz().decoder.to(device)\n",
    "\n",
    "enc_n_latents = 750\n",
    "alm_n_latents = enc_n_latents // (model.window_size)\n",
    "\n",
    "audios = defaultdict(dict)\n",
    "\n",
    "trajectories = [trajectory1, trajectory2, trajectory3]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        # Compute EnCodec audio\n",
    "        vector_list = create_vectors(trajectory, df_synth.loc[\"EnCodec\"], X_synth.loc[\"EnCodec\"])\n",
    "        print([np.isnan(v).any().item() for v in vector_list])\n",
    "        seq = interpolate_latent(vector_list, n=enc_n_latents)\n",
    "        seq = seq.unsqueeze(0)\n",
    "        seq = torch.tensor(seq, dtype=torch.float32).to(device)\n",
    "        seq = seq.permute(0, 2, 1)\n",
    "        x_hat = encodec_dec(seq).flatten()\n",
    "        audios[\"EnCodec\"][i] = x_hat.cpu().detach().numpy()\n",
    "\n",
    "        # Compute ALMTokenizer audio\n",
    "        vector_list = create_vectors(trajectory, df_synth.loc[\"ALMTokenizer\"], X_synth.loc[\"ALMTokenizer\"])\n",
    "        seq = interpolate_latent(vector_list, n=alm_n_latents)\n",
    "        seq = seq.unsqueeze(0)\n",
    "        seq = torch.tensor(seq).to(model.device)\n",
    "        x_hat = model.decode(seq).flatten()\n",
    "        audios[\"ALMTokenizer\"][i] = x_hat.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "\n",
    "mel_transform = MelSpectrogram()\n",
    "\n",
    "for method, audiodict in audios.items():\n",
    "    for i, audio in audiodict.items():\n",
    "        print(f\"Playing audio {i} for {method}:\")\n",
    "        display(Audio(audio, autoplay=True, rate=24000))\n",
    "        # Save audio\n",
    "        torchaudio.save(f\"docs/audio/{method}_trajectory_{i}.wav\", torch.tensor(audio).unsqueeze(0), 24000)\n",
    "        spec = mel_transform(torch.tensor(audio))\n",
    "        spec = spec.squeeze(1)\n",
    "        plt.imshow(spec.detach().cpu(), aspect='auto', origin='lower')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd34822",
   "metadata": {},
   "source": [
    "# Timbre Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "from utils import timbre_transfer\n",
    "\n",
    "\n",
    "move_to = {\n",
    "    \"instrument\": [\"trumpet\"],\n",
    "    \"note\": [\"A\"],\n",
    "\n",
    "    }\n",
    "\n",
    "before_encodec, wav_encodec, before_alm, wav_alm = timbre_transfer(model, os.path.join(\"docs/audio\", \"speech-male.wav\"), move_to, X_synth, df_synth, device=device)\n",
    "torchaudio.save(\"docs/audio/before_encodec_1.wav\", before_encodec.cpu(), 24000)\n",
    "torchaudio.save(\"docs/audio/after_encodec_1.wav\", wav_encodec.cpu(), 24000)\n",
    "torchaudio.save(\"docs/audio/before_alm_1.wav\", before_alm.cpu(), 24000)\n",
    "torchaudio.save(\"docs/audio/after_alm_1.wav\", wav_alm.cpu(), 24000)\n",
    "\n",
    "before_encodec, wav_encodec, before_alm, wav_alm = timbre_transfer(model, os.path.join(\"docs/audio\", \"speech-female.wav\"), move_to, X_synth, df_synth, device=device)\n",
    "torchaudio.save(\"docs/audio/before_encodec_2.wav\", before_encodec.cpu(), 24000)\n",
    "torchaudio.save(\"docs/audio/after_encodec_2.wav\", wav_encodec.cpu(), 24000)\n",
    "torchaudio.save(\"docs/audio/before_alm_2.wav\", before_alm.cpu(), 24000)\n",
    "torchaudio.save(\"docs/audio/after_alm_2.wav\", wav_alm.cpu(), 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
